packages = c("dplyr","ggplot2","rtweet" ,"xml2", "httr", "jsonlite", "data.tree", "NLP", "igraph","sentimentr","tidytext","wordcloud2","DiagrammeR","dplyr" , "showtext")
existing = as.character(installed.packages()[,1])
for(pkg in packages[!(packages %in% existing)]) install.packages(pkg)
library(wordcloud2)
library(ggplot2)
library(scales)
library(rtweet)
library(dplyr)
library(xml2)
library(httr)
library(jsonlite)
library(magrittr)
library(data.tree)
library(tidytext)
library(stringr)
library(DiagrammeR)
library(magrittr)
library(showtext)
showtext_auto()
font_add("PingFangSC-Regular",regular = "/System/Library/Fonts/PingFang.ttc")
app = 'heminghsin'
consumer_key = 'kjJGO9cTWdoCG9BHgBwfFtcfi'
consumer_secret = 'zjamiuhUuWZjjbsi01Jlg38uwVyXeQpiuFkUDk6QhiSIC379UO'
access_token = '1380751997343698949-qYevxQu1xqqP4dYegz4VeZUzSASi44'
access_secret = 'Lnbft6o7jK2Du8X9087qTFRRhs3UA2coYJSRi4KnrbR9d'
twitter_token <- create_token(app,consumer_key, consumer_secret,
access_token, access_secret,set_renv = FALSE)
#Consumer Keys:知道你的身分
#Authentication Tokens:認證給你的授權
head(df)
nrow(df)
min(df$created_at)
app = 'heminghsin'
consumer_key = 'kjJGO9cTWdoCG9BHgBwfFtcfi'
consumer_secret = 'zjamiuhUuWZjjbsi01Jlg38uwVyXeQpiuFkUDk6QhiSIC379UO'
access_token = '1380751997343698949-qYevxQu1xqqP4dYegz4VeZUzSASi44'
access_secret = 'Lnbft6o7jK2Du8X9087qTFRRhs3UA2coYJSRi4KnrbR9d'
twitter_token <- create_token(app,consumer_key, consumer_secret,
access_token, access_secret,set_renv = FALSE)
#Consumer Keys:知道你的身分
#Authentication Tokens:認證給你的授權
# 查詢關鍵字
key = c("tokyo")
context = "Olympics"
q = paste(c(key,context),collapse=" AND ")
# 查詢字詞 "#tokyo AND Olympics"
# 為了避免只下#tokyo 會找到非在Olympics中的tweets，加入Olympics要同時出現的條件
#抓5000筆 不抓轉推
tweets = search_tweets(q,lang="en",n=5000,include_rts = FALSE,token = twitter_token)
## 用於資料清理
clean = function(txt) {
txt = iconv(txt, "latin1", "ASCII", sub="") #改變字的encoding
txt = gsub("(@|#)\\w+", "", txt) #去除@或#後有數字,字母,底線 (標記人名或hashtag)
txt = gsub("(http|https)://.*", "", txt) #去除網址(.:任意字元，*:0次以上)
txt = gsub("[ \t]{2,}", "", txt) #去除兩個以上空格或tab
txt = gsub("\\n"," ",txt) #去除換行
txt = gsub("\\s+"," ",txt) #去除一個或多個空格(+:一次以上)
txt = gsub("^\\s+|\\s+$","",txt) #去除開頭/結尾有一個或多個空格
txt = gsub("&.*;","",txt) #去除html特殊字元編碼
txt = gsub("[^a-zA-Z0-9?!. ']","",txt) #除了字母,數字空白?!.的都去掉(表情符號去掉)
txt }
tweets$text = clean(tweets$text)  #text套用資料清理
df = data.frame()
df = rbind(df,tweets)  # transfer to data frame
df = df[!duplicated(df[,"status_id"]),]  #去除重複的tweets
head(df)
nrow(df)
min(df$created_at)
max(df$created_at)
# 產生coreNLP的api url，將本地端的網址轉成符合coreNLP服務的url
generate_API_url <- function(host, port="9000",
tokenize.whitespace="false", annotators=""){ #斷詞依據不是空格
url <- sprintf('http://%s:%s/?properties={"tokenize.whitespace":"%s","annotators":"%s"}', host, port, tokenize.whitespace, annotators)
url <- URLencode(url)
}
#指定服務的位置
host = "127.0.0.1"
generate_API_url(host)
# 呼叫coreNLP api
call_coreNLP <- function(server_host, text, host="localhost", language="eng",
tokenize.whitespace="true", ssplit.eolonly="true", annotators=c("tokenize","ssplit","pos","lemma","ner","parse","sentiment")){
# 假設有兩個core-nlp server、一個負責英文（使用9000 port）、另一個則負責中文（使用9001 port）
port <- ifelse(language=="eng", 9000, 9001);
# 產生api網址
url <- generate_API_url(server_host, port=port,
tokenize.whitespace=tokenize.whitespace, annotators=paste0(annotators, collapse = ','))
result <- POST(url, body = text, encode = "json")
doc <- httr::content(result, "parsed","application/json",encoding = "UTF-8")
return (doc)
}
#文件使用coreNLP服務
coreNLP <- function(data,host){
# 依序將每個文件丟進core-nlp進行處理，每份文件的回傳結果為json格式
# 在R中使用objects來儲存處理結果
result <- apply(data, 1 , function(x){
object <- call_coreNLP(host, x['text'])
list(doc=object, data=x)
})
return(result)
}
coreNLP_tokens_parser <- function(coreNLP_objects){
result <- do.call(rbind, lapply(coreNLP_objects, function(obj){
original_data <- obj$data
doc <- obj$doc
# for a sentences
sentences <- doc$sentences
sen <- sentences[[1]]
tokens <- do.call(rbind, lapply(sen$tokens, function(x){
result <- data.frame(word=x$word, lemma=x$lemma, pos=x$pos, ner=x$ner)
result
}))
tokens <- original_data %>%
t() %>%
data.frame() %>%
select(-text) %>%
slice(rep(1:n(), each = nrow(tokens))) %>%
bind_cols(tokens)
tokens
}))
return(result)
}
coreNLP_dependency_parser <- function(coreNLP_objects){
result <- do.call(rbind, lapply(coreNLP_objects, function(obj){
original_data <- obj$data
doc <- obj$doc
# for a sentences
sentences <- doc$sentences
sen <- sentences[[1]]
dependencies <- do.call(rbind, lapply(sen$basicDependencies, function(x){
result <- data.frame(dep=x$dep, governor=x$governor, governorGloss=x$governorGloss, dependent=x$dependent, dependentGloss=x$dependentGloss)
result
}))
dependencies <- original_data %>%
t() %>%
data.frame() %>%
select(-text) %>%
slice(rep(1:n(), each = nrow(dependencies))) %>%
bind_cols(dependencies)
dependencies
}))
return(result)
}
coreNLP_sentiment_parser <- function(coreNLP_objects){
result <- do.call(rbind, lapply(coreNLP_objects, function(obj){
original_data <- obj$data
doc <- obj$doc
# for a sentences
sentences <- doc$sentences
sen <- sentences[[1]]
sentiment <- original_data %>%
t() %>%
data.frame() %>%
bind_cols(data.frame(sentiment=sen$sentiment, sentimentValue=sen$sentimentValue))
sentiment
}))
return(result)
}
# 圖形化顯示dependency結果
parse2tree <- function(ptext) {
stopifnot(require(NLP) && require(igraph))
# this step modifies coreNLP parse tree to mimic openNLP parse tree
ptext <- gsub("[\r\n]", "", ptext)
ptext <- gsub("ROOT", "TOP", ptext)
## Replace words with unique versions
ms <- gregexpr("[^() ]+", ptext)                                      # just ignoring spaces and brackets?
words <- regmatches(ptext, ms)[[1]]                                   # just words
regmatches(ptext, ms) <- list(paste0(words, seq.int(length(words))))  # add id to words
## Going to construct an edgelist and pass that to igraph
## allocate here since we know the size (number of nodes - 1) and -1 more to exclude 'TOP'
edgelist <- matrix('', nrow=length(words)-2, ncol=2)
## Function to fill in edgelist in place
edgemaker <- (function() {
i <- 0                                       # row counter
g <- function(node) {                        # the recursive function
if (inherits(node, "Tree")) {            # only recurse subtrees
if ((val <- node$value) != 'TOP1') { # skip 'TOP' node (added '1' above)
for (child in node$children) {
childval <- if(inherits(child, "Tree")) child$value else child
i <<- i+1
edgelist[i,1:2] <<- c(val, childval)
}
}
invisible(lapply(node$children, g))
}
}
})()
## Create the edgelist from the parse tree
edgemaker(Tree_parse(ptext))
tree <- FromDataFrameNetwork(as.data.frame(edgelist))
return (tree)
}
gc() #釋放不使用的記憶體
t0 = Sys.time()
obj = df[,c(2,5)]  %>% filter(text != "") %>% coreNLP(host) #丟入本地執行
gc() #釋放不使用的記憶體
t0 = Sys.time()
obj = df[,c(2,5)]  %>% filter(text != "") %>% coreNLP(host) #丟入本地執行
tokens =  coreNLP_tokens_parser(obj)
gc() #釋放不使用的記憶體
t0 = Sys.time()
obj = df[,c(2,5)]  %>% filter(text != "") %>% coreNLP(host) #丟入本地執行
sentiment %>%
merge(df[,c("status_id","source","date")]) %>%
filter(source %in% c("Twitter Web Client","Twitter for iPhone","Twitter for Android")) %>%
group_by(date,source) %>%
summarise(avg_sentiment = mean(sentimentValue,na.rm=T)) %>%
ggplot(aes(x=date,y=avg_sentiment,color=source)) +
geom_line()
gc() #釋放不使用的記憶體
t0 = Sys.time()
obj = df[,c(2,5)]  %>% filter(text != "") %>% coreNLP(host) #丟入本地執行
gc() #釋放不使用的記憶體
t0 = Sys.time()
obj = df[,c(2,5)]  %>% filter(text != "") %>% coreNLP(host) #丟入本地執行
packages = c("pacman")
existing = as.character(installed.packages()[,1])
for(pkg in packages[!(packages %in% existing)]) install.packages(pkg)
Sys.setlocale(category = "LC_ALL", locale = "Chinese (Traditional)_Taiwan.950")
# 避免中文亂碼zh_TW.UTF-8
pacman::p_load("dplyr", "tidytext", "jiebaR", "gutenbergr", "stringr", "wordcloud2", "ggplot2", "tidyr", "scales","widyr","ggraph", "igraph","cnSentimentR","data.table","glmnet","broom","rsample","caTools","caret","rpart","rpart.plot","e1071","textstem","tm", 'readr','reshape2','wordcloud',"corrplot","Hmisc","fmsb","GGally","ggrepel","BiocManager")
stock=fread("C:\\Users\\Davis Liu\\Documents\\R\\SOCIAL MEDIA ANALYSIS\\midterm\\articleMetaData1.csv",header=TRUE,sep=",")
head(stock)
# 格式化日期欄位
stock$artDate= stock$artDate %>% as.Date("%Y/%m/%d")
stock
stock_n_bydate<-stock %>%
transform(artCat = as.factor(artCat)) %>%
group_by(artDate,artCat) %>%
summarise(count=n(), total_comment=sum(commentNum))
packages = c("pacman")
existing = as.character(installed.packages()[,1])
for(pkg in packages[!(packages %in% existing)]) install.packages(pkg)
Sys.setlocale(category = "LC_ALL", locale = "zh_TW.UTF-8") # 避免中文亂碼
# 避免中文亂碼zh_TW.UTF-8
pacman::p_load("dplyr", "tidytext", "jiebaR", "gutenbergr", "stringr", "wordcloud2", "ggplot2", "tidyr", "scales","widyr","ggraph", "igraph","cnSentimentR","data.table","glmnet","broom","rsample","caTools","caret","rpart","rpart.plot","e1071","textstem","tm", 'readr','reshape2','wordcloud',"corrplot","Hmisc","fmsb","GGally","ggrepel","BiocManager", "showtext")
library(showtext)
showtext_auto()
font_add("PingFangSC-Regular",regular = "/System/Library/Fonts/PingFang.ttc")
stock=fread("C:\\Users\\Davis Liu\\Documents\\R\\SOCIAL MEDIA ANALYSIS\\midterm\\articleMetaData1.csv",header=TRUE,sep=",")
head(stock)
# 格式化日期欄位
stock$artDate= stock$artDate %>% as.Date("%Y/%m/%d")
stock
stock_n_bydate<-stock %>%
transform(artCat = as.factor(artCat)) %>%
group_by(artDate,artCat) %>%
summarise(count=n(), total_comment=sum(commentNum))
packages = c("pacman")
existing = as.character(installed.packages()[,1])
for(pkg in packages[!(packages %in% existing)]) install.packages(pkg)
Sys.setlocale(category = "LC_ALL", locale = "zh_TW.UTF-8") # 避免中文亂碼
# 避免中文亂碼zh_TW.UTF-8
pacman::p_load("dplyr", "tidytext", "jiebaR", "gutenbergr", "stringr", "wordcloud2", "ggplot2", "tidyr", "scales","widyr","ggraph", "igraph","cnSentimentR","data.table","glmnet","broom","rsample","caTools","caret","rpart","rpart.plot","e1071","textstem","tm", 'readr','reshape2','wordcloud',"corrplot","Hmisc","fmsb","GGally","ggrepel","BiocManager", "showtext")
library(showtext)
showtext_auto()
font_add("PingFangSC-Regular",regular = "/System/Library/Fonts/PingFang.ttc")
stock=fread("/Users/marco/Documents/File/中山大學/社群媒體/期中報告/PTT 股票板_articleMetaData2.csv",header=TRUE,sep=",")
head(stock)
# 格式化日期欄位
stock$artDate= stock$artDate %>% as.Date("%Y/%m/%d")
stock
stock_n_bydate<-stock %>%
transform(artCat = as.factor(artCat)) %>%
group_by(artDate,artCat) %>%
summarise(count=n(), total_comment=sum(commentNum))
stock_n_bydate
stock_n_bydate_plot<-stock_n_bydate %>%
ggplot(aes(x=reorder(artDate, artDate),y=count, fill=artCat)) +
geom_bar(position = 'identity', stat = "identity")+
#geom_label_repel(min.segment.length = 0, box.padding = 0.5)+
theme(axis.text.x = element_text(angle = 90))+
ggtitle("每日文章量")
stock_n_bydate_plot
jieba_tokenizer <- worker()
# 設定斷詞function
stock_tokenizer <- function(t) {
lapply(t, function(x) {
tokens <- segment(x, jieba_tokenizer)
return(tokens)
})
}
#filter<-c("the","and","you")
tokens_stock <- stock %>% unnest_tokens(word, sentence, token=stock_tokenizer) #%>%
#filter_segment(filter) %>%
#str()
tokens_stock
tokens_stock_filter <- tokens_stock %>%
filter(!grepl('[[:punct:]]',word)) %>% # 去標點符號
filter(!grepl("['^0-9a-z']",word)) %>% # 去英文、數字
filter(nchar(.$word)>1)
stock_word_count <- tokens_stock_filter %>%
group_by(word,artDate,artCat) %>%
summarise(word_count=n()) %>%  # 算字詞單篇總數用summarise
filter(word_count>3) %>%  # 過濾出現太少次的字
arrange(desc(word_count))
stock_word_count
stock_word_count_plot<-stock_word_count %>%
head(20) %>%
arrange(desc(word_count)) %>%
ggplot(aes(x=reorder(word, word_count),y=word_count, fill=artCat)) +
geom_col() +
xlab(NULL) +
coord_flip()+
ggtitle("top10 word")
stock_word_count_plot
stock_word_count_plot_bystock<-stock_word_count %>%
arrange(desc(word_count)) %>%
head(30) %>%
ggplot(aes(x=reorder(word, word_count),y=word_count)) +
facet_wrap(~artCat, scales = "free") +
geom_col() +
xlab(NULL) +
coord_flip()
ggtitle("top10 word")
stock_word_count_plot_bystock
#install.packages("wordcloud2")
library(wordcloud2)
cloud_word_count <- tokens_stock_filter %>%
group_by(word) %>%
summarise(word_count=n()) %>%
filter(word_count>3) %>%
arrange(desc(word_count))
wordcloud2(cloud_word_count)
dtm = stock_word_count %>%
cast_dtm(artDate,word,word_count)
inspect(dtm[1:10,1:10])
word_cors <- tokens_stock %>%
group_by(word) %>%
filter(n() >= 10) %>%
pairwise_cor(word, artDate, sort = TRUE)
word_cors
set.seed(2021)
word_cors %>%
filter(correlation > 0.8) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 3) +
geom_node_text(aes(label = name), repel = TRUE, family = "Heiti TC Light") + #加入中文字型設定，避免中文字顯示錯誤。
theme_void()
tokens_stock_addtfidf<- stock_word_count%>%
bind_tf_idf(word,artCat,word_count)%>%
arrange(desc(tf_idf))
tokens_stock_addtfidf
tokens_stock_addtfidf_plot <- tokens_stock_addtfidf %>%
head(15) %>%
ggplot(aes(x=reorder(word, tf_idf),y=tf_idf, fill=artCat)) +
geom_col() +
xlab(NULL) +
coord_flip()
tokens_stock_addtfidf_plot
dtm_tfidf<-tokens_stock_addtfidf %>%
cast_dtm(artDate,word,tf_idf)
inspect(dtm_tfidf[1:10,1:10])
P <- read_file("/Users/marco/Documents/File/中山大學/社群媒體/期中報告/positive.txt") # 正向字典txt檔
N <- read_file("/Users/marco/Documents/File/中山大學/社群媒體/期中報告/negative.txt") # 負向字典txt檔
# 將字串依,分割
# strsplit回傳list , 我們取出list中的第一個元素
P = strsplit(P, ",")[[1]]
N = strsplit(N, ",")[[1]]
# 建立dataframe 有兩個欄位word,sentiments，word欄位內容是字典向量
P = data.frame(word = P, sentiment = "positive") #664
N = data.frame(word = N, sentiment = "negative") #1047
# 把兩個字典拼在一起
LIWC = rbind(P, N)
# 檢視字典
head(LIWC)
stock_sentiment_count <-stock_word_count %>%
#select(word,chapter) %>%
inner_join(LIWC) %>%
group_by(sentiment,artDate) %>%
summarise(sentiment_number = n())
stock_sentiment_count$sentiment_total = stock_sentiment_count$sentiment_number * ifelse(stock_sentiment_count$sentiment == "positive", 1, -1)
stock_sentiment_count
stock_sentiment_count_plot<-stock_sentiment_count %>%
ggplot(aes(x=reorder(artDate, artDate),y=sentiment_total, fill=sentiment)) +
geom_bar(position = 'identity', stat = "identity")+
theme(axis.text.x = element_text(angle = 90))
stock_sentiment_count_plot
stock_sentiment_count_total<-stock_sentiment_count %>%
group_by(artDate) %>%
summarise(sentiment_sum=sum(sentiment_total))
stock_sentiment_count_total
stock_sentiment_count_total_plot<-stock_sentiment_count_total %>%
ggplot(aes(x=reorder(artDate, artDate),y=sentiment_sum)) +
geom_bar(position = 'identity', stat = "identity")+
theme(axis.text.x = element_text(angle = 90))
stock_sentiment_count_total_plot
dtm_mod <- as.data.frame(as.matrix(dtm)) %>%
cbind(artDate = rownames(.), .) %>%
transform(artDate = as.Date(artDate))
rownames(dtm_mod) <- 1:nrow(dtm_mod)
dtm_mod
dtm_tfidf_mod = as.data.frame(as.matrix(dtm_tfidf)) %>%
cbind(artDate = rownames(.), .) %>%
transform(artDate = as.Date(artDate))
rownames(dtm_tfidf_mod) <- 1:nrow(dtm_tfidf_mod)
dtm_tfidf_mod
history=fread("/Users/marco/Documents/File/中山大學/社群媒體/期中報告/2610_history.csv",header=TRUE,sep=",")
head(history)
# 格式化日期欄位
history$artDate= history$Date %>% as.Date("%Y/%m/%d") %>% +1
history$Date= history$Date %>% as.Date("%Y/%m/%d")
history
#合併股票資料歷史資料
combine<- history %>%
left_join(.,dtm_mod, by=("artDate")) %>%
left_join(.,dtm_tfidf_mod, by=("artDate")) %>%
left_join(.,stock_sentiment_count_total, by=("artDate")) %>%
arrange(desc((artDate)))
combine
#合併股票資料、字頻TF-IDF、DTM資料與歷史資料
target<-combine %>%
select(Date,Close,Change) %>%
rename(target_price = Close,predict_date=Date,predict_change=Change)
combine_addtarget<-combine %>%
inner_join(target,by=c("artDate"="predict_date")) %>%
replace(is.na(.), 0)
combine_addtarget
combine_addtarget_input<-combine_addtarget %>%
subset(.,select=-c(Date,artDate,predict_change))
lm_model <- glm(formula=target_price~., data = combine_addtarget_input)
summary(lm_model)
combine_upordown_input<-combine_addtarget %>%
mutate(up_down = ifelse(predict_change > 0, 1, 0)) %>%
subset(.,select=-c(Date,artDate,target_price,predict_change))
combine_upordown_input$up_down
lm_model_pn_binary <- glm(formula=up_down~., data = combine_upordown_input,family = "binomial")
summary(lm_model_pn_binary)
lm_model_confirm <- lm(formula=combine_addtarget$target_price~確診.x, data = combine_addtarget_input)
summary(lm_model_confirm)
combine_addtarget$pred_outcome<-predict(lm_model_confirm,newdata = combine_addtarget_input)
error_plot_confirm<- combine_addtarget %>%
ggplot(aes(x=target_price,y=pred_outcome )) +
geom_point(color = 'red') +
geom_smooth(method=lm)
error_plot_confirm
lm_model_confirm <- lm(formula=combine_addtarget$target_price~sentiment_sum, data = combine_addtarget_input)
summary(lm_model_confirm)
combine_addtarget$pred_outcome<-predict(lm_model_confirm,newdata = combine_addtarget_input)
error_plot_confirm<- combine_addtarget %>%
ggplot(aes(x=target_price,y=pred_outcome )) +
geom_point(color = 'red') +
geom_smooth(method=lm)
error_plot_confirm
#合併股票資料、字頻TF-IDF、DTM資料與歷史資料
target<-combine %>%
select(Date,Close,Change) %>%
rename(target_price = Close,predict_date=Date,predict_change=Change)
